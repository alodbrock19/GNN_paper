{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e8fdece",
   "metadata": {},
   "source": [
    "# TGCN Stock Trend Classification - Complete Training Pipeline\n",
    "\n",
    "A clean, well-organized notebook for training Temporal Graph Convolutional Networks (TGCN) to predict stock trends using inter-stock relationships.\n",
    "\n",
    "**Overview:**\n",
    "- Data: 100 stocks with OHLCV data and hybrid graph structure\n",
    "- Task: 3-class classification (Down/Neutral/Up) based on return thresholds\n",
    "- Method: TGCN with optional Focal Loss for class imbalance mitigation\n",
    "- Metrics: Accuracy, Precision, Recall, F1-Score with detailed visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67fecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 7: RESULTS SUMMARY AND EXPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create summary report\n",
    "summary_report = f\"\"\"\n",
    "{'='*70}\n",
    "TGCN STOCK TREND CLASSIFICATION - TRAINING SUMMARY\n",
    "{'='*70}\n",
    "\n",
    "RUN INFORMATION\n",
    "{'='*70}\n",
    "Run Name: {run_name}\n",
    "Run Directory: {run_dir}\n",
    "Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "DATASET CONFIGURATION\n",
    "{'='*70}\n",
    "- Total samples: {len(dataset)}\n",
    "- Training samples: {len(train_dataset)}\n",
    "- Test samples: {len(test_dataset)}\n",
    "- Batch size: {batch_size}\n",
    "- Stocks per sample: {dataset[0].num_nodes}\n",
    "- Input features: {in_channels}\n",
    "\n",
    "CLASS DISTRIBUTION (Test Set)\n",
    "{'='*70}\n",
    "- Down: {actual_counts[0]:5d} samples ({actual_counts[0]/len(actuals_flat)*100:5.1f}%)\n",
    "- Neutral: {actual_counts[1]:5d} samples ({actual_counts[1]/len(actuals_flat)*100:5.1f}%)\n",
    "- Up: {actual_counts[2]:5d} samples ({actual_counts[2]/len(actuals_flat)*100:5.1f}%)\n",
    "- Threshold: ±{threshold*100:.2f}%\n",
    "\n",
    "MODEL ARCHITECTURE\n",
    "{'='*70}\n",
    "- Model: {model.__class__.__name__}\n",
    "- Input channels: {in_channels}\n",
    "- Output channels: {out_channels}\n",
    "- Hidden size: {hidden_size}\n",
    "- Number of layers: {layers_nb}\n",
    "- Graph attention: {use_gat}\n",
    "- Total parameters: {sum(p.numel() for p in model.parameters()):,}\n",
    "\n",
    "TRAINING CONFIGURATION\n",
    "{'='*70}\n",
    "- Loss function: {loss_name}\n",
    "- Optimizer: Adam\n",
    "- Learning rate: {lr}\n",
    "- Weight decay: {weight_decay}\n",
    "- Number of epochs: {num_epochs}\n",
    "- Focal Loss: {use_focal_loss}\n",
    "\n",
    "RESULTS (TEST SET)\n",
    "{'='*70}\n",
    "Overall Accuracy: {accuracy:.2%}\n",
    "\n",
    "Per-Class Metrics:\n",
    "  DOWN:\n",
    "    - Precision: {report['Down']['precision']:.3f}\n",
    "    - Recall: {report['Down']['recall']:.3f}\n",
    "    - F1-Score: {report['Down']['f1-score']:.3f}\n",
    "  \n",
    "  NEUTRAL:\n",
    "    - Precision: {report['Neutral']['precision']:.3f}\n",
    "    - Recall: {report['Neutral']['recall']:.3f}\n",
    "    - F1-Score: {report['Neutral']['f1-score']:.3f}\n",
    "  \n",
    "  UP:\n",
    "    - Precision: {report['Up']['precision']:.3f}\n",
    "    - Recall: {report['Up']['recall']:.3f}\n",
    "    - F1-Score: {report['Up']['f1-score']:.3f}\n",
    "\n",
    "Macro Averaged Metrics:\n",
    "  - Precision: {report['macro avg']['precision']:.3f}\n",
    "  - Recall: {report['macro avg']['recall']:.3f}\n",
    "  - F1-Score: {report['macro avg']['f1-score']:.3f}\n",
    "\n",
    "TRAINING HISTORY\n",
    "{'='*70}\n",
    "- Initial training loss: {train_losses[0]:.4f}\n",
    "- Final training loss: {train_losses[-1]:.4f}\n",
    "- Initial test loss: {test_losses[0]:.4f}\n",
    "- Final test loss: {test_losses[-1]:.4f}\n",
    "- Loss improvement: {(train_losses[0] - train_losses[-1])/train_losses[0]*100:.1f}%\n",
    "\n",
    "FILES SAVED\n",
    "{'='*70}\n",
    "- Model: {model_save_path}\n",
    "- Evaluation visualization: {run_dir}/evaluation_results.png\n",
    "- Training history plot: {run_dir}/training_history.png\n",
    "- Summary report: {run_dir}/summary_report.txt\n",
    "\n",
    "{'='*70}\n",
    "END OF REPORT\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "\n",
    "# Save summary report\n",
    "report_path = f\"{run_dir}/summary_report.txt\"\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(summary_report)\n",
    "print(f\"\\n✓ Summary report saved to: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8265897",
   "metadata": {},
   "source": [
    "## Section 8: Results Summary and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09726cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training and Test Loss\n",
    "ax = axes[0]\n",
    "ax.plot(train_losses, label='Training Loss', linewidth=2, color='steelblue')\n",
    "ax.plot(test_losses, label='Test Loss', linewidth=2, color='coral')\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('Training History - Loss', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Loss improvement\n",
    "ax = axes[1]\n",
    "ax.bar(['Initial', 'Final'], [train_losses[0], train_losses[-1]], \n",
    "       width=0.35, label='Training', alpha=0.8, color='steelblue')\n",
    "ax.bar([1.35], [test_losses[-1]], width=0.35, label='Test', alpha=0.8, color='coral')\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('Loss Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks([0, 1, 1.35])\n",
    "ax.set_xticklabels(['Initial Train', 'Final Train', 'Final Test'])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{run_dir}/training_history.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"✓ Training history saved to: {run_dir}/training_history.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3c87d7",
   "metadata": {},
   "source": [
    "## Section 7: Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdac555",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 6: VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(18, 14))\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "cm = confusion_matrix(actuals_flat, preds_flat, labels=[0, 1, 2])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
    "            xticklabels=['Down', 'Neutral', 'Up'],\n",
    "            yticklabels=['Down', 'Neutral', 'Up'])\n",
    "ax1.set_title('Confusion Matrix (Test Set)', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Actual', fontsize=12)\n",
    "ax1.set_xlabel('Predicted', fontsize=12)\n",
    "\n",
    "# 2. Prediction Distribution\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "pred_counts = np.bincount(preds_flat.astype(int), minlength=3)\n",
    "actual_counts = np.bincount(actuals_flat.astype(int), minlength=3)\n",
    "x = np.arange(3)\n",
    "width = 0.35\n",
    "ax2.bar(x - width/2, actual_counts, width, label='Actual', alpha=0.8, color='steelblue')\n",
    "ax2.bar(x + width/2, pred_counts, width, label='Predicted', alpha=0.8, color='coral')\n",
    "ax2.set_xlabel('Class', fontsize=12)\n",
    "ax2.set_ylabel('Count', fontsize=12)\n",
    "ax2.set_title('Distribution of Predictions vs Actual', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(['Down', 'Neutral', 'Up'])\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Probability Distributions by Class\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "ax3.hist(probs_down[actuals_flat == 0], bins=30, alpha=0.5, label='Down (actual)', color='red', edgecolor='black')\n",
    "ax3.hist(probs_neutral[actuals_flat == 1], bins=30, alpha=0.5, label='Neutral (actual)', color='gray', edgecolor='black')\n",
    "ax3.hist(probs_up[actuals_flat == 2], bins=30, alpha=0.5, label='Up (actual)', color='green', edgecolor='black')\n",
    "ax3.set_xlabel('Prediction Probability', fontsize=12)\n",
    "ax3.set_ylabel('Frequency', fontsize=12)\n",
    "ax3.set_title('Probability Distribution by Class', fontsize=14, fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# 4. Returns vs Predictions\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "returns_flat = test_data.returns.squeeze().cpu().numpy().flatten()\n",
    "colors_map = {0: 'red', 1: 'gray', 2: 'green'}\n",
    "labels_map = {0: 'Down', 1: 'Neutral', 2: 'Up'}\n",
    "for class_idx in [0, 1, 2]:\n",
    "    mask = (preds_flat == class_idx)\n",
    "    if mask.sum() > 0:\n",
    "        ax4.scatter(returns_flat[mask], probs_max[mask],\n",
    "                   alpha=0.5, label=f'Predicted {labels_map[class_idx]}',\n",
    "                   color=colors_map[class_idx], s=20)\n",
    "ax4.axvline(x=-threshold, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax4.axvline(x=threshold, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax4.axvline(x=0, color='black', linestyle='-', linewidth=1, alpha=0.3)\n",
    "ax4.set_xlabel('Actual Return', fontsize=12)\n",
    "ax4.set_ylabel('Max Prediction Probability', fontsize=12)\n",
    "ax4.set_title('Returns vs Prediction Confidence', fontsize=14, fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(alpha=0.3)\n",
    "\n",
    "# 5. Accuracy by Return Range\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "return_bins = np.linspace(returns_flat.min(), returns_flat.max(), 10)\n",
    "bin_indices = np.digitize(returns_flat, return_bins)\n",
    "bin_accuracies = []\n",
    "bin_centers = []\n",
    "for i in range(1, len(return_bins)):\n",
    "    mask = (bin_indices == i)\n",
    "    if mask.sum() > 0:\n",
    "        bin_acc = (preds_flat[mask] == actuals_flat[mask]).mean()\n",
    "        bin_accuracies.append(bin_acc)\n",
    "        bin_centers.append((return_bins[i-1] + return_bins[i]) / 2)\n",
    "ax5.plot(bin_centers, bin_accuracies, marker='o', linewidth=2, markersize=8, color='steelblue')\n",
    "ax5.axhline(y=1/3, color='red', linestyle='--', linewidth=1, alpha=0.7, label='Random (33.3%)')\n",
    "ax5.set_xlabel('Return Range', fontsize=12)\n",
    "ax5.set_ylabel('Accuracy', fontsize=12)\n",
    "ax5.set_title('Model Accuracy by Return Range', fontsize=14, fontweight='bold')\n",
    "ax5.legend()\n",
    "ax5.grid(alpha=0.3)\n",
    "ax5.set_ylim([0, 1])\n",
    "\n",
    "# 6. Classification Report\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "ax6.axis('off')\n",
    "report_text = f\"\"\"\n",
    "CLASSIFICATION REPORT\n",
    "\n",
    "Overall Accuracy: {accuracy:.2%}\n",
    "\n",
    "DOWN (0):\n",
    "  Precision: {report['Down']['precision']:.3f}\n",
    "  Recall: {report['Down']['recall']:.3f}\n",
    "  F1-Score: {report['Down']['f1-score']:.3f}\n",
    "\n",
    "NEUTRAL (1):\n",
    "  Precision: {report['Neutral']['precision']:.3f}\n",
    "  Recall: {report['Neutral']['recall']:.3f}\n",
    "  F1-Score: {report['Neutral']['f1-score']:.3f}\n",
    "\n",
    "UP (2):\n",
    "  Precision: {report['Up']['precision']:.3f}\n",
    "  Recall: {report['Up']['recall']:.3f}\n",
    "  F1-Score: {report['Up']['f1-score']:.3f}\n",
    "\n",
    "MACRO AVG:\n",
    "  Precision: {report['macro avg']['precision']:.3f}\n",
    "  Recall: {report['macro avg']['recall']:.3f}\n",
    "  F1-Score: {report['macro avg']['f1-score']:.3f}\n",
    "\"\"\"\n",
    "ax6.text(0.1, 0.5, report_text, fontsize=10, family='monospace',\n",
    "         verticalalignment='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{run_dir}/evaluation_results.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Visualization saved to: {run_dir}/evaluation_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ba2c9e",
   "metadata": {},
   "source": [
    "## Section 6: Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526fd45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 5: MODEL EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Evaluation on test set\n",
    "model.eval()\n",
    "test_data = next(iter(test_dataloader))\n",
    "test_data = test_data.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(test_data.x, test_data.edge_index, test_data.edge_weight)\n",
    "    probs = F.softmax(logits, dim=-1)  # Softmax for 3-class\n",
    "    predictions = probs.argmax(dim=-1)\n",
    "    actual = test_data.y.squeeze()\n",
    "\n",
    "# Flatten for metrics\n",
    "preds_flat = predictions.cpu().numpy().flatten()\n",
    "actuals_flat = actual.cpu().numpy().flatten()\n",
    "probs_max = probs.max(dim=-1)[0].cpu().numpy().flatten()\n",
    "\n",
    "# Get per-class probabilities\n",
    "probs_down = probs[:, 0].cpu().numpy().flatten()\n",
    "probs_neutral = probs[:, 1].cpu().numpy().flatten()\n",
    "probs_up = probs[:, 2].cpu().numpy().flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = (preds_flat == actuals_flat).mean()\n",
    "report = classification_report(actuals_flat, preds_flat, \n",
    "                               target_names=['Down', 'Neutral', 'Up'],\n",
    "                               output_dict=True)\n",
    "\n",
    "print(f\"\\n✓ Test Set Evaluation:\")\n",
    "print(f\"  - Overall Accuracy: {accuracy:.2%}\")\n",
    "print(f\"\\n  Per-Class Metrics:\")\n",
    "for class_name in ['Down', 'Neutral', 'Up']:\n",
    "    p = report[class_name]['precision']\n",
    "    r = report[class_name]['recall']\n",
    "    f1 = report[class_name]['f1-score']\n",
    "    print(f\"    {class_name:8s}: Precision={p:.3f}, Recall={r:.3f}, F1={f1:.3f}\")\n",
    "\n",
    "print(f\"\\n  Macro Average: Precision={report['macro avg']['precision']:.3f}, \"\n",
    "      f\"Recall={report['macro avg']['recall']:.3f}, F1={report['macro avg']['f1-score']:.3f}\")\n",
    "\n",
    "# Class distribution analysis\n",
    "print(f\"\\n✓ Prediction vs Actual Distribution:\")\n",
    "actual_counts = np.bincount(actuals_flat, minlength=3)\n",
    "pred_counts = np.bincount(preds_flat, minlength=3)\n",
    "for i, name in enumerate(['Down', 'Neutral', 'Up']):\n",
    "    print(f\"  {name:8s}: Actual={actual_counts[i]:5d} ({actual_counts[i]/len(actuals_flat)*100:5.1f}%), \"\n",
    "          f\"Predicted={pred_counts[i]:5d} ({pred_counts[i]/len(preds_flat)*100:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebe0540",
   "metadata": {},
   "source": [
    "## Section 5: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67144092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_save_path = f\"models/saved_models/{run_name}_{model.__class__.__name__}.pt\"\n",
    "os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"\\n✓ Model saved to: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9569e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4: TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create run directory for saving results\n",
    "run_name = f\"TGCN_3class_{datetime.now().strftime('%d_%m_%Hh%M')}\"\n",
    "run_dir = f\"runs/{run_name}\"\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\n✓ Run directory: {run_dir}\")\n",
    "print(f\"\\nStarting training... (this may take a while)\")\n",
    "\n",
    "# Train model\n",
    "train_losses, test_losses = train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    num_epochs=num_epochs,\n",
    "    task_title=f\"TGCN_3class\",\n",
    "    measure_acc=True\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Training complete!\")\n",
    "print(f\"  - Final training loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"  - Final test loss: {test_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56dbaff",
   "metadata": {},
   "source": [
    "## Section 4: Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691ff825",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: MODEL ARCHITECTURE AND HYPERPARAMETERS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Model architecture parameters\n",
    "in_channels = dataset[0].x.shape[-2]  # Input feature dimension\n",
    "out_channels = 3                        # 3-class output (Down, Neutral, Up)\n",
    "hidden_size = 16                        # Hidden state size for TGCN\n",
    "layers_nb = 2                           # Number of TGCN layers\n",
    "use_gat = True                          # Use GAT instead of GCN\n",
    "\n",
    "# Training hyperparameters\n",
    "lr = 0.005                              # Learning rate\n",
    "weight_decay = 1e-5                     # L2 regularization\n",
    "num_epochs = 100                        # Number of training epochs\n",
    "use_focal_loss = True                   # Use Focal Loss for class imbalance\n",
    "\n",
    "print(f\"\\n✓ Model Architecture:\")\n",
    "print(f\"  - Input channels: {in_channels}\")\n",
    "print(f\"  - Output channels: {out_channels}\")\n",
    "print(f\"  - Hidden size: {hidden_size}\")\n",
    "print(f\"  - Layers: {layers_nb}\")\n",
    "print(f\"  - Graph attention: {use_gat}\")\n",
    "\n",
    "print(f\"\\n✓ Training Hyperparameters:\")\n",
    "print(f\"  - Learning rate: {lr}\")\n",
    "print(f\"  - Weight decay (L2): {weight_decay}\")\n",
    "print(f\"  - Epochs: {num_epochs}\")\n",
    "print(f\"  - Focal Loss: {use_focal_loss}\")\n",
    "\n",
    "# Initialize model\n",
    "model = TGCN(\n",
    "    in_channels=in_channels,\n",
    "    out_channels=out_channels,\n",
    "    hidden_size=hidden_size,\n",
    "    layers_nb=layers_nb,\n",
    "    use_gat=use_gat\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Model initialized: {model.__class__.__name__}\")\n",
    "print(f\"  - Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Define loss function\n",
    "if use_focal_loss:\n",
    "    class_weights = torch.tensor([0.3, 1.0, 0.3], dtype=torch.float32)\n",
    "    criterion = FocalLoss(alpha=class_weights, gamma=2.0)\n",
    "    loss_name = \"Focal Loss\"\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss_name = \"CrossEntropyLoss\"\n",
    "\n",
    "print(f\"\\n✓ Loss function: {loss_name}\")\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "print(f\"✓ Optimizer: Adam\")\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device) if hasattr(criterion, 'to') else criterion\n",
    "print(f\"\\n✓ Model moved to device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87ea36f",
   "metadata": {},
   "source": [
    "## Section 3: Model Architecture and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d9ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and test\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: TRAIN/TEST SPLIT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "train_ratio = 0.9\n",
    "batch_size = 32\n",
    "\n",
    "train_size = int(train_ratio * len(dataset))\n",
    "train_dataset = dataset[:train_size]\n",
    "test_dataset = dataset[train_size:]\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "\n",
    "print(f\"\\n✓ Train/test split:\")\n",
    "print(f\"  - Training samples: {len(train_dataset)} ({train_ratio*100:.1f}%)\")\n",
    "print(f\"  - Test samples: {len(test_dataset)} ({(1-train_ratio)*100:.1f}%)\")\n",
    "print(f\"  - Batch size: {batch_size}\")\n",
    "print(f\"  - Train batches: {len(train_dataloader)}\")\n",
    "print(f\"  - Test batches: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeb5983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels_3class(sample: Data, threshold: float = 0.0055):\n",
    "    \"\"\"\n",
    "    Transform stock returns into 3-class labels: Down, Neutral, Up\n",
    "    \n",
    "    Args:\n",
    "        sample: Graph data sample from dataset\n",
    "        threshold: Threshold for neutral zone (default: ±0.55%)\n",
    "    \n",
    "    Returns:\n",
    "        sample with y labels (0=Down, 1=Neutral, 2=Up) and returns\n",
    "    \"\"\"\n",
    "    # Calculate market return (average return across all stocks)\n",
    "    market_return = ((sample.close_price_y[:, -1] - sample.close_price[:, -1]) / sample.close_price[:, -1]).mean()\n",
    "    \n",
    "    # Store individual returns for analysis\n",
    "    sample.returns = ((sample.close_price_y[:, -1] - sample.close_price[:, -1]) / sample.close_price[:, -1]).unsqueeze(1)\n",
    "    sample.market_return = market_return\n",
    "    \n",
    "    # Create 3-class labels\n",
    "    sample.y = torch.zeros_like(sample.returns, dtype=torch.long).squeeze(1)\n",
    "    sample.y[sample.returns.squeeze(1) < -threshold] = 0      # Down\n",
    "    sample.y[(sample.returns.squeeze(1) >= -threshold) & \n",
    "             (sample.returns.squeeze(1) <= threshold)] = 1     # Neutral\n",
    "    sample.y[sample.returns.squeeze(1) > threshold] = 2        # Up\n",
    "    \n",
    "    return sample\n",
    "\n",
    "\n",
    "# Data loading parameters\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: DATA LOADING AND PREPROCESSING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "threshold = 0.0055  # ±0.55% threshold for neutral zone\n",
    "weeks_ahead = 1\n",
    "adj_file = 'hybrid_adj.npy'  # Use hybrid adjacency matrix\n",
    "\n",
    "print(f\"\\n✓ Parameters:\")\n",
    "print(f\"  - Threshold: ±{threshold*100:.2f}%\")\n",
    "print(f\"  - Prediction window: {weeks_ahead} week(s) ahead\")\n",
    "print(f\"  - Graph structure: {adj_file}\")\n",
    "\n",
    "# Create dataset with label transformation\n",
    "transform = partial(create_labels_3class, threshold=threshold)\n",
    "dataset = SP100Stocks(\n",
    "    root=\"data/\",\n",
    "    adj_file_name=adj_file,\n",
    "    future_window=weeks_ahead,\n",
    "    force_reload=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Dataset loaded: {len(dataset)} samples, {dataset[0].num_nodes} stocks\")\n",
    "\n",
    "# Analyze class distribution\n",
    "all_labels = []\n",
    "for i in range(len(dataset)):\n",
    "    all_labels.extend(dataset[i].y.cpu().numpy())\n",
    "\n",
    "unique, counts = np.unique(all_labels, return_counts=True)\n",
    "class_names = ['Down (0)', 'Neutral (1)', 'Up (2)']\n",
    "\n",
    "print(f\"\\n✓ Class distribution (overall):\")\n",
    "for label, count in zip(unique, counts):\n",
    "    pct = count / len(all_labels) * 100\n",
    "    print(f\"  {class_names[label]}: {count:5d} samples ({pct:5.2f}%)\")\n",
    "\n",
    "# Verify sample data\n",
    "print(f\"\\n✓ Sample verification (first 5 stocks from first sample):\")\n",
    "sample = dataset[0]\n",
    "for i in range(5):\n",
    "    label = int(sample.y[i].item())\n",
    "    ret = sample.returns[i].item() * 100\n",
    "    print(f\"  Stock {i}: return={ret:+7.2f}%, label={class_names[label]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff386f2f",
   "metadata": {},
   "source": [
    "## Section 2: Data Preprocessing and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c572778a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alrodriguezg/U/s5/gnn/GNN_paper/code/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TGCN STOCK TREND CLASSIFICATION - TRAINING PIPELINE\n",
      "======================================================================\n",
      "\n",
      "✓ Device: cuda\n",
      "✓ Configuration complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from tools.SP100Stock import SP100Stocks\n",
    "from models import TGCN, train, measure_accuracy\n",
    "from models.train import FocalLoss\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "print(\"=\"*70)\n",
    "print(\"TGCN STOCK TREND CLASSIFICATION - TRAINING PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\n✓ Device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (16, 12)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"✓ Configuration complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de5f002",
   "metadata": {},
   "source": [
    "## Section 1: Import Libraries and Configuration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
